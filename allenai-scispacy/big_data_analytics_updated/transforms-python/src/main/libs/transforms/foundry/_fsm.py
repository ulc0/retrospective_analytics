# Copyright 2017 Palantir Technologies, Inc.
import json
import logging

import pyspark

from transforms import _java_utils as j
from transforms import _java_version

log = logging.getLogger(__name__)


class FoundrySparkManager(j.JavaProxy):
    """Python proxy object around a FoundrySparkManager."""

    def __init__(self, service_provider, branches=None):
        jservice_config = self._parse_service_configs(service_provider.services_config)
        juser_agent = self._jvm.com.palantir.conjure.java.api.config.service.UserAgent.of(
            self._jvm.com.palantir.conjure.java.api.config.service.UserAgent.Agent.of(
                "transforms-python-foundry-spark-manager", _java_version.__version__
            )
        )
        jfsm = (
            self._jvm.com.palantir.foundry.spark.api.FoundrySparkManager.builder()
            .withDataSourceV2ReadFormats(j.to_string_array(["parquet", "csv"]))
            .services(jservice_config)
            .userAgent(juser_agent)
            .udfWhiteListPolicy(
                self._jvm.com.palantir.foundry.spark.api.UdfWhitelistPolicy.WHITELIST_ALL
            )
            .build(branches or [])
        )

        super().__init__(jfsm)

    def _parse_service_configs(self, service_confs):
        jcls = self._jvm.Class.forName(
            "com.palantir.conjure.java.api.config.service.ServiceConfiguration"
        )
        return {
            name: j.object_mapper().readValue(json.dumps(conf), jcls)
            for name, conf in service_confs.items()
        }

    def read_table(
        self,
        rid,
        to_version=None,
        from_version=None,
        branch=None,
        as_of=None,
        incremental_mode=None,
        additional_options=None,
    ):
        """Python wrapper for FoundrySparkManager readTable method.

        Args:
            rid (str): The resource identifier of the table.
            to_version (dict, optional): The resolved toVersion of the table (generated by the
                Tables Input Manager), corresponding to the current view of the table.
            from_version (dict, optional): The resolved fromVersion of the table (generated
                by the Tables Input Manager), used as the previous view of the table when
                reading from it incrementally. If absent, the provided table does not
                support incremental reading.
            branch (str, optional): The branch from which to read the table.
            as_of (int, optional): The instant, in milliseconds from the epoch of
                1970-01-01T00:00:00Z, at which to read the table. Note that this
                is the same unit as a table's timestamp and in other internal table
                metadata/operations.
            incremental_mode (str, optional): The mode to use when reading incrementally.
            options (dict, optional): Options to pass to the underlying Spark reader.

        Returns:
            FoundrySparkTable: the table object.
        """
        jbuilder = self._jvm.com.palantir.foundry.spark.api.TableReadRequest.builder()

        jbuilder.tableRid(j.to_rid(rid))
        jbuilder.branch(j.to_optional(branch))
        jbuilder.asof(j.to_optional(as_of, j.to_instant))

        jbuilder.fromVersion(j.to_optional(from_version, j.to_resolved_version))
        jbuilder.toVersion(j.to_optional(to_version, j.to_resolved_version))
        jbuilder.incrementalMode(j.to_optional(incremental_mode, j.to_incremental_mode))

        if additional_options is not None:
            for key, value in additional_options.items():
                jbuilder.putAdditionalOptions(key, value)

        return FoundrySparkTable(
            self._proxy.readTable(jbuilder.build()), self.spark_session
        )

    def read_dataset(
        self, rid, end_txrid=None, start_txrid=None, branch=None, schema_version=None
    ):
        """Python wrapper for FoundrySparkManager readDataset method.

        Args:
            rid (str): The resource identifier of the dataset.
            end_txrid (str, optional): The transaction resource identifier for the end of the view.
            start_txrid (str, optional): The transaction resource identifier for the start of the view.
            branch (str, optional): The branch from which to read the Foundry schema.
            schema_version (str, optional): The version of the schema to read.

        Returns:
            FoundrySparkDataset: the dataset object.
        """
        jbuilder = self._jvm.com.palantir.foundry.spark.api.DatasetReadRequest.builder()

        jbuilder.datasetRid(j.to_rid(rid))

        jbuilder.endTransactionRid(j.to_optional(end_txrid, j.to_rid))
        jbuilder.startTransactionRid(j.to_optional(start_txrid, j.to_rid))
        jbuilder.branchId(j.to_optional(branch))
        jbuilder.schemaVersionId(j.to_optional(schema_version))

        return FoundrySparkDataset(
            self._proxy.readDataset(jbuilder.build()), self.spark_session
        )

    def write_dataset(
        self,
        df,
        rid,
        txrid=None,
        branch=None,
        mode=None,
        filesystem_id=None,
        partition_cols=None,
        bucket_cols=None,
        bucket_count=None,
        sort_by=None,
        output_format=None,
        options=None,
    ):
        """Python wrapper for FoundrySparkManager writerDataset method.

        Args:
            df (pyspark.sql.DataFrame): The DataFrame to write.
            rid (str): the resource identifier of the dataset.
            txrid (str, optional): The transaction resource identifier to write to, otherwise a
                new SNAPSHOT transaction is created.
            branch (str, optional): The branch to write to, otherwise the first fallback branch is chosen.
            mode (str, optional): The write mode to use, one of 'replace' or 'modify'
            filesystem_id (str, optional): The filesystem ID to write to for new datasets.
            partition_cols (List[str], optional): Column partitioning to use when writing data.
            bucket_cols (List[str], optional): The columns by which to bucket the data.
            bucket_count (int, optional): The number of buckets. Must be specified if bucket_cols or sort_by is given.
            sort_by (List[str], optional): The columns by which to sort the bucketed data.
            output_format (str, optional): The output file format, defaults to 'parquet'.
            options (dict, optional): Extra options to pass through to
                ``org.apache.spark.sql.DataFrameWriter#option(String, String)``.

        Returns:
            FoundrySparkDataset: the updated dataset object.
        """
        write_modes = {
            "replace": self._jvm.com.palantir.foundry.spark.api.FoundrySparkWriteMode.SNAPSHOT,
            "modify": self._jvm.com.palantir.foundry.spark.api.FoundrySparkWriteMode.APPEND,
            "append": self._jvm.com.palantir.foundry.spark.api.FoundrySparkWriteMode.STRICT_APPEND,
        }

        if mode and mode not in write_modes:
            raise ValueError(f"Mode {mode} must be one of {write_modes.keys()}")

        jbuilder = (
            self._jvm.com.palantir.foundry.spark.api.DatasetWriteRequest.builder()
        )

        jbuilder.dataset(df._jdf)
        jbuilder.datasetRid(j.to_rid(rid))

        jbuilder.transactionRid(j.to_optional(txrid, j.to_rid))
        jbuilder.branchId(j.to_optional(branch))
        jbuilder.writeMode(j.to_optional(mode, lambda m: write_modes[m]))
        jbuilder.fileSystemId(j.to_optional(filesystem_id))

        jformatBuilder = (
            self._jvm.com.palantir.foundry.spark.api.DatasetFormatSettings.builder()
        )
        jformatBuilder.partitionColumns(j.to_list(partition_cols or []))
        jformatBuilder.bucketColumns(j.to_list(bucket_cols or []))
        jformatBuilder.numBuckets(j.to_optional(bucket_count))
        jformatBuilder.sortColumns(j.to_list(sort_by or []))
        jformatBuilder.format(j.to_optional(output_format))
        jformatBuilder.options(options or {})

        jbuilder.formatSettings(jformatBuilder.build())

        return FoundrySparkDataset(
            self._proxy.writeDataset(jbuilder.build()), self.spark_session
        )

    @property
    def spark_session(self):
        """py4j.sql.SparkSession: Get a handle to the Spark session in use.

        We have to wrap the spark session that FoundrySparkManager has constructed for us.
        """
        sc = pyspark.SparkContext.getOrCreate()
        jspark_session = self._proxy.sparkSession()
        return pyspark.sql.SparkSession(sc, jsparkSession=jspark_session)


class FoundrySparkTable(j.JavaProxy):
    """Python proxy object around a FoundrySparkTable."""

    def __init__(self, jproxy, spark_session):
        """Wrap a FoundrySparkTable also providing the SparkSession handle."""
        super().__init__(jproxy)
        self._spark_session = spark_session

    def table(self):
        """Get a Dataframe for the FoundrySparkTable.

        Returns:
            pyspark.sql.DataFrame: The DataFrame for the dataset.
        """
        sc = pyspark.SparkContext.getOrCreate()
        sql_context = pyspark.sql.SQLContext(sc, sparkSession=self._spark_session)
        return pyspark.sql.DataFrame(self._proxy.table(), sql_context)


class FoundrySparkDataset(j.JavaProxy):
    """Python proxy object around a FoundrySparkDataset."""

    def __init__(self, jproxy, spark_session):
        """Wrap a FoundrySparkDataset also providing the SparkSession handle."""
        super().__init__(jproxy)
        self._spark_session = spark_session

    def dataset(self):
        """Get a Dataframe for the FoundrySparkDataset.

        Returns:
            pyspark.sql.DataFrame: The DataFrame for the dataset.
        """
        sc = pyspark.SparkContext.getOrCreate()
        sql_context = pyspark.sql.SQLContext(sc, sparkSession=self._spark_session)
        return pyspark.sql.DataFrame(self._proxy.dataset(), sql_context)

    def dataset_path(self):
        """Get the dataset's Compass path.

        Returns:
            str: The compass path for the dataset.
        """
        return self._proxy.datasetPath()

    def schema_version(self):
        """Get the schema version.

        Returns:
            str: The new Foundry schema version.
        """
        optional_version = self._proxy.foundrySchemaVersion()
        if not optional_version.isPresent():
            log.info("Schema version not found on FoundrySparkDataset object")
            return None
        return optional_version.get()


class FoundrySparkDatasetSaver(j.JavaProxy):
    """Python proxy object around a FoundrySparkDatasetSaver."""

    def set_dataset_rid(self, rid):
        """Sets the dataset rid to save in. Cannot be called if ``set_dataset_path`` has already been called.

        Args:
            rid (str): The dataset resource identifier.
        """
        self._proxy.setDatasetRid(j.to_rid(rid))

    def set_branch_id(self, branch_id):
        """Sets the branchId to save on.

        Branch must already exist. If unspecified, will use the FoundrySparkManager's default branch id.

        Args:
            branch_id (str): The branch identifier.
        """
        self._proxy.setBranchId(branch_id)

    def set_filesystem_id(self, filesystem_id):
        """Sets file system identifier.

        Only needs to be set when creating a new dataset (that doesn't exist yet).

        Args:
            filesystem_id (str): The filesystem ID

        """
        self._proxy.setFileSystemId(filesystem_id)

    def save_within_open_transaction(self, txrid):
        """Saves the dataset within a currently open transaction.

        Args:
            txrid (str): The transaction resource identifier.
        """
        return self._proxy.saveWithinOpenTransaction(
            j.to_rid(txrid)
        ).getSchemaVersionId()

    def save_dataset(self):
        """Saves the dataset in foundry. Will open and commit a new transaction on the given foundry dataset.

        Returns:
            str: The new schema version ID
        """
        return self._proxy.saveDataset().getSchemaVersionId()


class FoundryDataTypes(j.JavaProxy):
    """Python proxy object around FoundryDataTypes."""

    def __init__(self):
        super().__init__(
            j.get_or_create_gateway().jvm.com.palantir.foundry.spark.types.FoundryDataTypes
        )

    def get_spark_data_type(self, foundry_field_schema):
        """Take a FoundryFieldSchema and convert it into a Spark DataType.

        We do this in Java to make sure we use the same code path as all other languages.

        Returns:
            pyspark.sql.types.DataType: The Spark DataType
        """
        jcls = self._jvm.Class.forName(
            "com.palantir.foundry.schemas.api.types.FoundryFieldSchema"
        )
        type_str = (
            self._proxy.getSparkStructField(
                j.object_mapper().readValue(json.dumps(foundry_field_schema), jcls)
            )
            .dataType()
            .json()
        )
        return pyspark.sql.types._parse_datatype_json_string(type_str)
