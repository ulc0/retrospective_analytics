{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6597c2b6-b9de-44dd-a418-ab081f60c1c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id\",  \n",
    "               dbutils.secrets.get(scope=\"dbs-scope-prod-kv-CDH\", key=\"cdh-adb-client-id\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret\", \n",
    "               dbutils.secrets.get(scope=\"dbs-scope-prod-kv-CDH\", key=\"cdh-adb-client-secret\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\",\n",
    "                dbutils.secrets.get(scope=\"dbs-scope-prod-kv-CDH\", key=\"cdh-adb-tenant-id-endpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f2dd89-18d8-4751-bbfb-daee5880940d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a565b7-a989-443a-a527-61eec8770529",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab565237-a294-458c-a47e-8fe0acd9e9f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "notes0 = spark.table(\"cdh_abfm_phi_exploratory.mpoxproject_data_set_9_9_23_run9\")\n",
    "#spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "#import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import isnan, when, count, col, desc, asc, row_number, concat_ws, coalesce,to_timestamp,regexp_replace, to_date, date_format, concat, lower, udf\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "#   RTF from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions',7200*4)\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True)\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, desc, asc, row_number, concat_ws, coalesce,to_timestamp,regexp_replace, to_date, date_format, concat\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "#setup sparknlp https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/sentence/sentence_detector/index.html\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.common import *\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "#https://medium.com/spark-nlp/cleaning-and-extracting-content-from-html-xml-documents-using-spark-nlp-documentnormalizer-913d96b2ee34\n",
    "\n",
    "\n",
    "# cleaning function for misc cleaning\n",
    "def deleteMarkup(colNote):                  \n",
    "     font_header1 = regexp_replace(colNote, \"Tahoma;;\", \"\")\n",
    "     font_header2 = regexp_replace(font_header1, \"Arial;Symbol;| Normal;heading 1;\", \"\")\n",
    "     font_header3 = regexp_replace(font_header2, \"Segoe UI;;\", \"\")\n",
    "     font_header4 = regexp_replace(font_header3, \"MS Sans Serif;;\", \"\")\n",
    "     return font_header4 \n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "\n",
    "clean_text = ( \n",
    "              # cleaning RFT here, document normalizer does not capture the RTF pattern. It is easier doing it here instead\n",
    "    lambda s: re.sub(r\"\\\\[a-z]+(-?\\d+)?[ ]?|\\\\'([a-fA-F0-9]{2})|[{}]|[\\n\\r]|\\r\\n?\", '', s)\n",
    ")\n",
    "\n",
    "notes = (\n",
    "    notes0\n",
    "    .withColumn('note_clean_rtf', udf(clean_text, StringType())('notes'))\n",
    "    .withColumn('note_clean_misc', deleteMarkup(F.col(\"note_clean_rtf\")))\n",
    ")\n",
    "\n",
    "\n",
    "#https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb\n",
    "\n",
    "documentAssembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol('note_clean_misc') \n",
    "    .setOutputCol('note_doc')\n",
    ")\n",
    "\n",
    "#default\n",
    "cleanUpPatternsHTML = [\"<[^>]*>|&[^;]+;\"]\n",
    "documentNormalizerHTML = (\n",
    "    DocumentNormalizer() \n",
    "    .setInputCols([\"note_doc\"]) \n",
    "    .setOutputCol(\"cleaned_markup\") \n",
    "    .setAction(\"clean\") \n",
    "    .setPatterns(cleanUpPatternsHTML) \n",
    "    .setReplacement(\" \") \n",
    "    .setPolicy(\"pretty_all\") \n",
    "    .setLowercase(True)\n",
    ")\n",
    "\n",
    "sentence = (\n",
    "    SentenceDetector() \n",
    "    .setInputCols([\"cleaned_markup\"]) \n",
    "    .setOutputCol(\"note_sent\") \n",
    "    .setExplodeSentences(True)\n",
    ") \n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"note_sent\"]) \\\n",
    "    .setOutputCols([\"note_string\"]) \\\n",
    "    .setOutputAsArray(False) \\\n",
    "    .setCleanAnnotations(False) \n",
    "\n",
    "\n",
    "docPatternRemoverPipeline = (\n",
    "    Pipeline() \n",
    "    .setStages([documentAssembler,                \n",
    "                documentNormalizerHTML,\n",
    "                sentence,\n",
    "                finisher\n",
    "                ]\n",
    "               )\n",
    ")\n",
    "\n",
    "treated_notes = docPatternRemoverPipeline.fit(notes).transform(notes)\n",
    "display(notes)\n",
    "#display(treated_notes.sort(\"person_id\",\"note_datetime\")) # sort seems to be an expensive operation\n",
    "display(treated_notes) # sort seems to be an expensive operation\n",
    "\n",
    "treated_notes.selectExpr(\"explode(note_sent) as sentences\").display()\n",
    "treated_notes.display()\n",
    "\n",
    "#sentences=pipeline.fit(notes).transform(notes)\n",
    "treated_notes.selectExpr(\"explode(cleaned_markup) as test\").show(truncate=False)\n",
    "\n",
    "#sentences.display()#show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f1bbeb-4670-4e47-b924-e98794eadb2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "082c062c-d230-4a98-b8d3-ce3e289984e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fs_abfm_notes_txt",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
