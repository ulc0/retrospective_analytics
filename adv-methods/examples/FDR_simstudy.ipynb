{
<<<<<<< Updated upstream
 "metadata": {
  "name": "",
  "signature": "sha256:07af45c236c7046e51064a9d68fb83a41d0f6089529b6868387914a6f2389263"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "False Discovery Rates\n",
      "=====================\n",
      "\n",
      "*Key ideas:* False discovery rates, simulation study, multiple testing\n",
      "    \n",
      "This notebook is a small simulation study to assess the sampling properties of different approaches to handling multiple testing, using the routines provided in the statsmodels `stats.multitest` module. All assessments are done for the case of a one-sample Z-test for the expected value of a population, where the data are an independent and identically distributed sample from the population.\n",
      "\n",
      "We start by importing a few modules:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from statsmodels.stats.multitest import multipletests\n",
      "from scipy.stats.distributions import norm\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we define a class that holds several parameters that control how the test data are generated.  Each parameter is given a default value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class parameters:\n",
      "\n",
      "    # Number of simulation replications\n",
      "    nrep = 1000\n",
      "\n",
      "    # Sample size\n",
      "    n = 40\n",
      "\n",
      "    # Effect size (mean under the alternative hypothesis).\n",
      "    effect_size = 1\n",
      "\n",
      "    # Cluster sie\n",
      "    clust_size = 5\n",
      " \n",
      "    # Intraclass correlation for clusters\n",
      "    icc = 0.\n",
      "\n",
      "    # The threshold for calling a positive result\n",
      "    threshold = 0.1\n",
      "\n",
      "    # The multitest method to evaluate\n",
      "    method = \"fdr_by\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we define a function called \"simulation\", where all the simulation takes place.  This function takes an instance of the \"parameters\" class as an argument. The results of this function are:\n",
      "    \n",
      "* Expected number of calls\n",
      "* FDR (False Discovery Rate) -- this is what the FDR methods are supposed to control"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def simulate(p):\n",
      "\n",
      "    efdr = []\n",
      "    for i in range(p.nrep):\n",
      "    \n",
      "        data = np.random.normal(size=(200,p.n))\n",
      "\n",
      "        # Introduce some positive dependence\n",
      "        data = np.kron(data, np.ones((p.clust_size,1)))\n",
      "        data = np.sqrt(p.icc)*data + np.sqrt(1-p.icc)*np.random.normal(size=data.shape)\n",
      "                \n",
      "        # 20 tests will follow the alternative hypothesis.  Place these\n",
      "        # tests at random positions so they are distributed among the clusters\n",
      "        ii = np.random.permutation(data.shape[0])\n",
      "        i1 = ii[:20]\n",
      "        i0 = ii[20:]\n",
      "        data[i1,:] += p.effect_size\n",
      "\n",
      "        # ix=1 is true alternative, ix=0 is true null\n",
      "        ix = np.zeros(data.shape[0])\n",
      "        ix[i1] = 1\n",
      "\n",
      "        # Carry out the one-sample Z-tests\n",
      "        zscores = np.sqrt(p.n) * data.mean(1) / data.std(1)\n",
      "        pvalues = 2*norm.cdf(-np.abs(zscores))\n",
      "    \n",
      "        # Get the adjusted test results\n",
      "        apv = multipletests(pvalues, method=p.method)[1]\n",
      "        \n",
      "        # Number of calls, empirical FDR\n",
      "        efdr.append([np.sum(apv < p.threshold), np.mean(ix[apv < p.threshold] == 0)])\n",
      "        \n",
      "    return np.asarray(efdr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we run the simulation with independent tests (all population structure parameters are set at their default values).  The results show that the approach is slightly conservative (the observed FDR is lower than the nominal FDR, which defaults to 0.1)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "efdr = simulate(parameters())\n",
      "rslt = pd.Series(efdr.mean(0), index=[\"Mean #calls\", \"Observed FDR\"])\n",
      "print rslt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean #calls     20.884000\n",
        "Observed FDR     0.045815\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we introduce some positive dependence among the tests.  The FDR approaches are derived for data-generating models with no dependence between tests, but the methods often perform well even when fairly strong dependence is present."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = parameters()\n",
      "p.icc = 0.9\n",
      "p.effect_size = 1\n",
      "efdr = simulate(p)\n",
      "rslt = pd.Series(efdr.mean(0), index=[\"Mean #calls\", \"Observed FDR\"])\n",
      "print rslt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean #calls     20.861000\n",
        "Observed FDR     0.041134\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}
=======
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99cce8a8-16b0-4767-a141-35be4233a075",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "False Discovery Rates\n",
    "=====================\n",
    "\n",
    "*Key ideas:* False discovery rates, simulation study, multiple testing\n",
    "    \n",
    "This notebook is a small simulation study to assess the sampling properties of different approaches to handling multiple testing, using the routines provided in the statsmodels `stats.multitest` module. All assessments are done for the case of a one-sample Z-test for the expected value of a population, where the data are an independent and identically distributed sample from the population.\n",
    "\n",
    "We start by importing a few modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a44ae5-dffc-4869-83e3-fafe8079c7d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats.distributions import norm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ecbf6ac-95c4-40b0-9285-824fa135809f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we define a class that holds several parameters that control how the test data are generated.  Each parameter is given a default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c97352-3735-4579-a63f-5b853170ec74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class parameters:\n",
    "\n",
    "    # Number of simulation replications\n",
    "    nrep = 1000\n",
    "\n",
    "    # Sample size\n",
    "    n = 40\n",
    "\n",
    "    # Effect size (mean under the alternative hypothesis).\n",
    "    effect_size = 1\n",
    "\n",
    "    # Cluster sie\n",
    "    clust_size = 5\n",
    " \n",
    "    # Intraclass correlation for clusters\n",
    "    icc = 0.\n",
    "\n",
    "    # The threshold for calling a positive result\n",
    "    threshold = 0.1\n",
    "\n",
    "    # The multitest method to evaluate\n",
    "    method = \"fdr_by\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa96b212-92ab-4d5e-999b-03ef6d6ee9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we define a function called \"simulation\", where all the simulation takes place.  This function takes an instance of the \"parameters\" class as an argument. The results of this function are:\n",
    "    \n",
    "* Expected number of calls\n",
    "* FDR (False Discovery Rate) -- this is what the FDR methods are supposed to control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb03a78-cb7c-4aba-8ed6-2313c74dd2cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def simulate(p):\n",
    "\n",
    "    efdr = []\n",
    "    for i in range(p.nrep):\n",
    "    \n",
    "        data = np.random.normal(size=(200,p.n))\n",
    "\n",
    "        # Introduce some positive dependence\n",
    "        data = np.kron(data, np.ones((p.clust_size,1)))\n",
    "        data = np.sqrt(p.icc)*data + np.sqrt(1-p.icc)*np.random.normal(size=data.shape)\n",
    "                \n",
    "        # 20 tests will follow the alternative hypothesis.  Place these\n",
    "        # tests at random positions so they are distributed among the clusters\n",
    "        ii = np.random.permutation(data.shape[0])\n",
    "        i1 = ii[:20]\n",
    "        i0 = ii[20:]\n",
    "        data[i1,:] += p.effect_size\n",
    "\n",
    "        # ix=1 is true alternative, ix=0 is true null\n",
    "        ix = np.zeros(data.shape[0])\n",
    "        ix[i1] = 1\n",
    "\n",
    "        # Carry out the one-sample Z-tests\n",
    "        zscores = np.sqrt(p.n) * data.mean(1) / data.std(1)\n",
    "        pvalues = 2*norm.cdf(-np.abs(zscores))\n",
    "    \n",
    "        # Get the adjusted test results\n",
    "        apv = multipletests(pvalues, method=p.method)[1]\n",
    "        \n",
    "        # Number of calls, empirical FDR\n",
    "        efdr.append([np.sum(apv < p.threshold), np.mean(ix[apv < p.threshold] == 0)])\n",
    "        \n",
    "    return np.asarray(efdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d96049-1984-4c32-bd33-9601fb50c7e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we run the simulation with independent tests (all population structure parameters are set at their default values).  The results show that the approach is slightly conservative (the observed FDR is lower than the nominal FDR, which defaults to 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31d45c5-4ad5-4b57-b42e-6c1f6089b046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "efdr = simulate(parameters())\n",
    "rslt = pd.Series(efdr.mean(0), index=[\"Mean #calls\", \"Observed FDR\"])\n",
    "print rslt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fda27be-2f22-4ae1-8667-b1fd53d3f550",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we introduce some positive dependence among the tests.  The FDR approaches are derived for data-generating models with no dependence between tests, but the methods often perform well even when fairly strong dependence is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04ea8790-704d-4a31-9e46-717fe177b47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "p = parameters()\n",
    "p.icc = 0.9\n",
    "p.effect_size = 1\n",
    "efdr = simulate(p)\n",
    "rslt = pd.Series(efdr.mean(0), index=[\"Mean #calls\", \"Observed FDR\"])\n",
    "print rslt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "FDR_simstudy",
   "widgets": {}
  },
  "name": "",
  "signature": "sha256:07af45c236c7046e51064a9d68fb83a41d0f6089529b6868387914a6f2389263"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
>>>>>>> Stashed changes
