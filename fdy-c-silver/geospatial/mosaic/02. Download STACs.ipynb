{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d9da0fd-7ddd-413f-a6cd-258ff051c0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download STAC Assets\n",
    "\n",
    "> For this demo we will require a few spatial libraries that can be easily installed via pip install. We will be using gdal, rasterio, pystac and databricks-mosaic for data download and data manipulation. We will use Microsoft [Planetary Computer](https://planetarycomputer.microsoft.com/) as the [STAC](https://stacspec.org/en) source of the raster data. __Note: Because we are using the free tier of MPC, downloads might be throttled.__ \n",
    "\n",
    "---\n",
    "__Last Update:__ 18 JAN 2024 [Mosaic 0.3.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6134b1d-c496-4cad-b718-e97161b7dd38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "<p/>\n",
    "\n",
    "1. Import Databricks columnar functions (including H3) for DBR with `from pyspark.databricks.sql.functions import *`\n",
    "2. To use Databricks Labs [Mosaic](https://databrickslabs.github.io/mosaic/index.html) library for geospatial data engineering, analysis, and visualization functionality:\n",
    "  * Configure Init Script to install GDAL on your cluster [[1](https://databrickslabs.github.io/mosaic/usage/install-gdal.html)]\n",
    "  * Install with `%pip install databricks-mosaic`\n",
    "  * Import and use with the following:\n",
    "  ```\n",
    "  import mosaic as mos\n",
    "  mos.enable_mosaic(spark, dbutils)\n",
    "  mos.enable_gdal(spark)\n",
    "  ```\n",
    "<p/>\n",
    "\n",
    "3. To use [KeplerGl](https://kepler.gl/) OSS library for map layer rendering:\n",
    "  * Already installed with Mosaic, use `%%mosaic_kepler` magic [[Mosaic Docs](https://databrickslabs.github.io/mosaicusage/kepler.html)]\n",
    "  * Import with `from keplergl import KeplerGl` to use directly\n",
    "\n",
    "If you have trouble with Volume access:\n",
    "\n",
    "* For Mosaic 0.3 series (< DBR 13)     - you can copy resources to DBFS as a workaround\n",
    "* For Mosaic 0.4 series (DBR 13.3 LTS) - you will need to either copy resources to DBFS or setup for Unity Catalog +Shared Access which will involve your workspace admin. Instructions, as updated, will be [here](https://databrickslabsgithub.io/mosaic/usage/install-gdal.html).\n",
    "\n",
    "The search and download phase was run on AWS [m5d.xlarge](https://www.databricks.com/product/pricing/product-pricing/instance-types) instances (2-16 workers auto-scaling for up to 64 concurrent downloads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76262388-e82f-478c-a875-70faffb1b6ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a05253-7af7-4d3f-952c-cbb22278ca23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet 'databricks-mosaic<0.4,>=0.3'\n",
    "%pip install --quiet rasterio==1.3.5 gdal==3.4.3 pystac pystac_client planetary_computer tenacity rich pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7505aa15-198d-42b8-85c6-6818a5fbf020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- configure AQE for more compute heavy operations\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 512)\n",
    "\n",
    "# -- import databricks + delta + spark functions\n",
    "from delta.tables import *\n",
    "from pyspark.databricks.sql import functions as dbf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# -- setup mosaic\n",
    "import mosaic as mos\n",
    "\n",
    "mos.enable_mosaic(spark, dbutils)\n",
    "mos.enable_gdal(spark)\n",
    "\n",
    "# -- other imports\n",
    "from datetime import datetime\n",
    "import library\n",
    "import os\n",
    "import pathlib\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c8769c6-050a-44c6-9477-9ab8b07ad3e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mos.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfccf39-0f52-4f91-8434-4ce772cc7087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "362f2a9c-834d-4b3b-9269-aecc2ae8dc41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks Catalog + Schema\n",
    "\n",
    "> This is for writing out table(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7842068-f169-4a16-8d1b-79bc373653f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adjust to your preferred catalog + schema\n",
    "catalog_name = \"geospatial_docs\"\n",
    "db_name = \"eo_alaska\"\n",
    "\n",
    "sql(f\"\"\"USE CATALOG {catalog_name}\"\"\")\n",
    "\n",
    "# uncomment to cleanup prior\n",
    "# sql(f\"\"\"DROP DATABASE IF EXISTS {db_name} CASCADE\"\"\")\n",
    "\n",
    "sql(f\"\"\"CREATE DATABASE IF NOT EXISTS {db_name}\"\"\")\n",
    "sql(f\"\"\"USE DATABASE {db_name}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2997dabb-085d-424b-be6b-302e6082cdd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data `ETL_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21ab71b-eeed-4f9a-898f-c0a65d319de2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust this path to suit your needs...\n",
    "user_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "ETL_DIR = f\"/home/{user_name}/stac/eo-series\"\n",
    "ETL_DIR_FUSE = f\"/dbfs/{ETL_DIR}\"\n",
    "\n",
    "os.environ['ETL_DIR'] = ETL_DIR\n",
    "os.environ['ETL_DIR_FUSE'] = ETL_DIR_FUSE\n",
    "\n",
    "# dbutils.fs.rm(ETL_DIR, True) # <- uncomment to clean out\n",
    "dbutils.fs.mkdirs(ETL_DIR)\n",
    "print(f\"...ETL_DIR: '{ETL_DIR}', ETL_DIR_FUSE: '{ETL_DIR_FUSE}' (create)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93230ce6-1fe6-428f-a5c3-fbd0430e5ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Alaska STAC Asset Download\n",
    "\n",
    "> We can easily extract the download links for items of interest. In this case, we will grab data within Alaska. Note: due to limitations in the [free tier for Planetary Computer](https://planetarycomputer.microsoft.com/docs/concepts/sas/), we will not attempt to get all available data within our time range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7284c2c8-86c9-4a0c-94df-8744fe0b789d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set EO_DIR to data/alaska subfolder of ETL_DIR\n",
    "EO_DIR = f\"{ETL_DIR}/data/alaska\"\n",
    "EO_DIR_FUSE = f\"/dbfs{EO_DIR}\"\n",
    "\n",
    "os.environ['EO_DIR'] = EO_DIR\n",
    "os.environ['EO_DIR_FUSE'] = EO_DIR_FUSE\n",
    "\n",
    "# dbutils.fs.rm(EO_DIR, True) # <- uncomment to clean out\n",
    "dbutils.fs.mkdirs(EO_DIR)\n",
    "print(f\"...EO_DIR: '{EO_DIR}', EO_DIR_FUSE: '{EO_DIR_FUSE}' (create)\")\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def file_size(file_path):\n",
    "  \"\"\"\n",
    "  Return file_size or null.\n",
    "  - must exist and be a file\n",
    "  \"\"\"\n",
    "  import os\n",
    "\n",
    "  if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "    return os.path.getsize(file_path)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def timestamp_filename(dt):\n",
    "  \"\"\"\n",
    "  Convert a passed timestamp to a filename friendly output.\n",
    "  - return looks like 20230131-092030\n",
    "  \"\"\"\n",
    "  from datetime import datetime\n",
    "\n",
    "  if dt is None:\n",
    "    return None\n",
    "  return dt.strftime(library.FILENAME_TIMESTAMP_FORMAT)\n",
    "\n",
    "def get_now_formatted():\n",
    "  \"\"\"\n",
    "  Use for last update.\n",
    "  - this is same as used in `timestamp_filename`\n",
    "  \"\"\"\n",
    "  return datetime.now().strftime(library.FILENAME_TIMESTAMP_FORMAT)\n",
    "\n",
    "def download_band(\n",
    "    eod_items, band_name, is_append_mode, tbl_prefix=\"band\", eo_dir=EO_DIR, repartition_factor=5,\n",
    "    do_clean_files=False, do_download=True, do_table_write=True\n",
    "  ):\n",
    "  \"\"\"\n",
    "  Download band into table.\n",
    "  - sets the 'last_update'\n",
    "  - assumes databricks catalog and schema already set\n",
    "  - default is append mode vs overwrite\n",
    "  - default is 'do_table_write=True'\n",
    "  - default is 'do_download=True'\n",
    "  - default is 'do_clean_files=False'\n",
    "  - filenames are '{band_name}_{item_id}.tif'\n",
    "  Returns dataframe either from table or generated.\n",
    "  !!! If you do not write, the returned dataframe will have not yet been executed !!!\n",
    "\n",
    "  Notes:\n",
    "  [a] It can take some time to clean files; this will be everything in the <band_name> dir\n",
    "  [b] It can take some time to download files per band, especially in MPC free tier\n",
    "  [c] If not doing table write, then the dataframe will not have forced execution on every row,\n",
    "      you will have to manage that as the caller\n",
    "  [d] You can change the table prefix to be more isolated for a given analytic / time filter\n",
    "  \"\"\"\n",
    "  _eod_items = eod_items.filter(f\"asset.name == '{band_name}'\")\n",
    "  orig_repart_num = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "  repart_num = round(_eod_items.count() / repartition_factor)\n",
    "  spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) # <- option-2: just tweak partition management\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", repart_num)\n",
    "  print(f\"\\t...shuffle partitions to {repart_num} for this operation.\")\n",
    "  try:\n",
    "    last_updated = get_now_formatted()\n",
    "\n",
    "    tbl_name = f\"{tbl_prefix}_{band_name}\"\n",
    "    eo_dir_band = f\"{eo_dir}/{band_name}\"\n",
    "\n",
    "    # [1] what are we downloading?\n",
    "    #     - see that asset_name is static\n",
    "    #     - band name will be used below\n",
    "    to_download = (\n",
    "      _eod_items\n",
    "        .repartition(repart_num)\n",
    "        .groupBy(\"item_id\", \"timestamp\")\n",
    "          .agg(\n",
    "            F.sort_array(F.collect_set(\"h3\")).alias(\"h3_set\"),\n",
    "            *[F.first(cn).alias(cn) for cn in eod_items.columns if cn not in [\"item_id\", \"timestamp\", \"h3\", \"geojson\"]]\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"band_name\",\n",
    "            F.lit(band_name)\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"out_dir_fuse\",\n",
    "            F.lit(f\"/dbfs{eo_dir_band}\")\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"out_filename\",\n",
    "            F.concat(\n",
    "              col(\"band_name\"), F.lit(\"_\"), col(\"item_id\"),\n",
    "              F.lit(\"_\"), timestamp_filename(\"timestamp\"), F.lit(\".tif\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\n",
    "          \"last_update\",\n",
    "          F.lit(last_updated)\n",
    "        )  \n",
    "    )\n",
    "\n",
    "    # [3] clean files?\n",
    "    if do_clean_files:\n",
    "      dbutils.fs.rm(eo_dir_band, True)\n",
    "\n",
    "    # [4] do download?\n",
    "    if do_download:\n",
    "      to_download = (\n",
    "        to_download\n",
    "          .withColumn(\n",
    "            \"out_file_path\", \n",
    "            library.download_asset(\n",
    "              \"item_id\",\n",
    "              \"band_name\",\n",
    "              \"out_dir_fuse\",\n",
    "              \"out_filename\"\n",
    "              )\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"out_file_sz\",\n",
    "            file_size(\"out_file_path\")\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"is_out_file_valid\",\n",
    "            F\n",
    "              .when(F.isnull(\"out_file_sz\"), F.lit(False))\n",
    "              .when(col(\"out_file_sz\") > F.lit(library.FILE_SIZE_THRESHOLD), F.lit(True))\n",
    "              .otherwise(F.lit(False))\n",
    "          )\n",
    "      )\n",
    "    else:\n",
    "      to_download = (\n",
    "        to_download\n",
    "          .withColumn(\n",
    "            \"out_file_path\", \n",
    "            F.concat(col(\"out_dir_fuse\"), F.lit(\"/\"), col(\"out_filename\")) # <- path set manually\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"out_file_sz\",\n",
    "            F.lit(None).cast(\"integer\") # <- sz unknown\n",
    "          )\n",
    "          .withColumn(\n",
    "            \"is_out_file_valid\",\n",
    "            F.lit(None).cast(\"boolean\") # <- validity unknown\n",
    "          )\n",
    "      )\n",
    "  \n",
    "    # [5] write table?\n",
    "    # [6] append mode?\n",
    "    if do_table_write:\n",
    "      write_mode = \"append\"\n",
    "      if not is_append_mode:\n",
    "        write_mode = \"overwrite\"\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {tbl_name}\")\n",
    "\n",
    "      (\n",
    "        to_download\n",
    "          .write\n",
    "            .mode(write_mode)\n",
    "          .saveAsTable(tbl_name)\n",
    "      )\n",
    "\n",
    "      # - return dataframe of written table\n",
    "      #   !!! this is fully executed !!!\n",
    "      return spark.table(tbl_name)\n",
    "    else:\n",
    "      # - return unwritten dataframe\n",
    "      #   !!! this is not executed yet !!!\n",
    "      return to_download\n",
    "  finally:\n",
    "    # print(f\"...setting shuffle partitions back to {orig_repart_num}\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", orig_repart_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be8ee5cf-1a21-4f64-914c-7fe3a2ca8b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CELL_ASSET_DIR_FUSE = None\n",
    "CELL_ASSET_DIR = None\n",
    "LAST_UPDATED = None\n",
    "\n",
    "# - previous write?\n",
    "for r in os.listdir(EO_DIR_FUSE):\n",
    "  if r.startswith(\"cell_assets_\"):\n",
    "    LAST_UPDATED = r.split('_')[-1]\n",
    "    CELL_ASSET_DIR_FUSE = f\"{EO_DIR_FUSE}/{r}\"\n",
    "    CELL_ASSET_DIR = CELL_ASSET_DIR_FUSE.replace('/dbfs','')\n",
    "    break\n",
    "\n",
    "os.environ['CELL_ASSET_DIR'] = CELL_ASSET_DIR\n",
    "os.environ['CELL_ASSET_DIR_FUSE'] = CELL_ASSET_DIR_FUSE\n",
    "\n",
    "print(f\"LAST_UPDATED: '{LAST_UPDATED}' [from working location] ->\")\n",
    "print(f\"\\tCELL_ASSET_DIR: '{CELL_ASSET_DIR}'\")\n",
    "print(f\"\\tCELL_ASSET_DIR_FUSE: '{CELL_ASSET_DIR_FUSE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93513e52-4b80-4e62-9a27-e2cd84d90fd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls -ls $EO_DIR_FUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf0373d-9045-41c3-8525-cc0ff621e642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eod_item_df = spark.read.load(CELL_ASSET_DIR)\n",
    "print(f\"count? {eod_item_df.count():,}\")\n",
    "eod_item_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28a7bbb1-b9d5-4b2d-a69c-f7bfa6c4b450",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Notice that some assets overlap more than one h3 cellid._ __Function `download_band` consolidates to unique 'item_id' values vs focus on h3 cell(s) to avoid repeated download requests.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e440ea-40df-4d4c-8387-977f9ff769a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# - notice multipe h3 cells for some item ids\n",
    "display (\n",
    "  eod_item_df\n",
    "    .filter(\"item_id == 'S2A_MSIL2A_20210601T204021_R014_T07VEK_20210602T071624'\")\n",
    "    .filter(\n",
    "        f\"asset.name == '{bands[0]}'\"\n",
    "      ) \n",
    "    .orderBy(\"h3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1cb0767-8f1c-4066-b3f6-a757e6df683d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download bands for items \n",
    "\n",
    "> This will generate a table per band in the specified catalog and schema (set earlier in the notebook). __Note: We are invoking with `do_clean_files=False` to avoid wiping out already downloaded files; also, passing `False` for 'is_append_mode' param on the table generation side, but you can pass `True` to change.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2869e3-3576-475b-a0b8-86422cf32507",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### First - a Dry-Run\n",
    "\n",
    "> Add all the columns that are added in the \"live\" execution, but we are specifying no actual execution (sanity check). __Note: we have nulls for `out_file_sz` (size) and `is_out_file_valid` since the files were not actually downloaded or checked.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a71d9d3-5e97-45cc-9108-7587c51e483e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# - First, a do-nothing dry-run for sanity check...\n",
    "# !!! NOTICE: do_clean_files, do_download, and do_table-write all FALSE !!!\n",
    "b_df_dry = download_band(eod_item_df, bands[0], False, do_clean_files=False, do_download=False, do_table_write=False)\n",
    "display(b_df_dry.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c62e1d-e428-43d9-946d-49120497967f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Download bands of interest\n",
    "\n",
    "> For this example series, we focus on B04 (red), B03 (green), B02 (blue), and B08 (nir). __You can easily download all / more.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa6815f-c9ad-4b91-9b1e-bf78bb3c7450",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Download Just 'B02'__\n",
    "\n",
    "> We have `do_clean_files=False` to not overwrite any existing data (for repeated execution). The 'band_b02' metadata table is set to be overwritten with `append_mode` set to `False`. __Note: you can adjust this to append vs overwrite.__ Also, it is ok to interrupt and restart execution as files are first checked to see if they have already been downloaded to avoid unnecessary IOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326d854f-9dba-46af-b10e-75b8f321e085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "download_band(eod_item_df, 'B02', False, do_clean_files=False, do_download=True, do_table_write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517aedd2-beb7-4287-9372-1e074c40e0e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Look at the band table generated for B02 [blue]._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077ee254-91e0-4b69-acaf-63d09df4e0cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * from band_b02 limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16d797aa-91e4-444d-a5a2-45542f968d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Look at a couple of the band GeoTIFFs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9018b880-56f8-4f01-b775-2e42ed5021dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex_bands = [t[0] for t in spark.table('band_b02').select('out_file_path').limit(2).collect()]\n",
    "display(\n",
    "  spark.table(\"band_b02\")\n",
    "  .where(\n",
    "    col(\"out_file_path\").isin(ex_bands)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1c0024-8aca-4eba-8b91-e524272d1e06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for b in ex_bands:\n",
    "  library.plot_file(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4eb200-9838-4138-8d11-0f42e6968ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"b02 total count? {sql(\"select format_number(count(1),0) from band_b02\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b02 valid count? {sql(\"select format_number(count(1),0) from band_b02 where is_out_file_valid\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b02 false count? {sql(\"select format_number(count(1),0) from band_b02 where is_out_file_valid = False\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b02 null count?  {sql(\"select format_number(count(1),0) from band_b02 where is_out_file_valid is null\").first()[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9312daff-9680-4bd7-bc97-63af1214a6cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Optional: Fix Missing Data\n",
    "\n",
    "> As a result of being gated in the free tier to Planetary Computure, a number of attempts to download band data might have resulted in an message versus the actual data (no failure condition provided). Here is what that might look like:\n",
    "\n",
    "```\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthenticationFailed</Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\n",
    "RequestId:bf21b919-d01e-002f-4e00-2ae765000000\n",
    "Time:2023-12-08T18:04:30.9365583Z</Message><AuthenticationErrorDetail>Signature not valid in the specified time frame: Start [Thu, 07 Dec 2023 17:18:15 GMT] - Expiry [Fri, 08 Dec 2023 18:03:15 GMT] - Current [Fri, 08 Dec 2023 18:04:30 GMT]</AuthenticationErrorDetail></Error>\n",
    "```\n",
    "\n",
    "The size is around 550 bytes, so we can test for this and smartly retry.\n",
    "\n",
    "__Note:__ We are using Delta Lake MERGE support to udate a given band table [[1](https://docs.databricks.com/en/delta/merge.html#language-python)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8992e6c0-7ee9-4e35-abb9-cffb3f87eba4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Since 'B02' (blue) now has all data, let's turn to 'B03' (green).__\n",
    "\n",
    "> Initially, we set `do_download=False` and `do_table_write` to demonstrate how this table can be filled in with a subsequent call to `download_missing_assets(...)` or `update_assets(...)`. __Note; 'out_file_sz' and 'is_out_file_valid' are both set to `Null` because we have not yet calculated this information (e.g. there may be some pre-existing files).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1373fc8d-2ec1-48db-9282-cf840531ed05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# - example of a table write without file download\n",
    "display(\n",
    "  download_band(eod_item_df, 'B03', False, do_clean_files=False, do_download=False, do_table_write=True)\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084acec9-490b-4c79-bba1-f2e2edb08edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- notice the table was written\n",
    "-- but no files downloaded\n",
    "select * from band_b03 limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8a0940-ed57-45d5-b525-6f685f5efbc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_See all for field 'is_out_file_valid' are `null`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75af9fbf-606d-410e-a9f7-0de8c8c6baf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"b03 total count? {sql(\"select format_number(count(1),0) from band_b03\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b03 valid count? {sql(\"select format_number(count(1),0) from band_b03 where is_out_file_valid = True\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b03 false count? {sql(\"select format_number(count(1),0) from band_b03 where is_out_file_valid = False\").first()[0]}\"\"\")\n",
    "print(f\"\"\"b03 null count?  {sql(\"select format_number(count(1),0) from band_b03 where is_out_file_valid is null\").first()[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5424cf8-3413-4ddc-a5de-5c67a8e33240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_assets(update_df, band_tbl_name):\n",
    "  \"\"\"\n",
    "  Test the out\n",
    "  - This expects an existing band tbl_name generated from `download_band(...)`\n",
    "  - This expects an update_df conforming to what is generated by `download_band(...)`\n",
    "    to include from `download_missing_assets(...)`\n",
    "  Returns a dataframe filtered to the merged data (from the band table).\n",
    "  \"\"\"\n",
    "  from datetime import datetime\n",
    "  \n",
    "  last_updated = get_now_formatted()\n",
    "  \n",
    "  # [1] udf for download_asset\n",
    "  # [2] re-calc size\n",
    "  # [3] re-calc valid\n",
    "  df = (\n",
    "    update_df\n",
    "      .drop(\n",
    "        \"last_update\",\n",
    "        \"out_file_path\", \n",
    "        \"out_file_sz\",\n",
    "        \"is_out_file_valid\"\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"last_update\",\n",
    "        F.lit(last_updated)\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"out_file_path\", \n",
    "        library.download_asset(\n",
    "          \"item_id\",\n",
    "          \"band_name\",\n",
    "          \"out_dir_fuse\",\n",
    "          \"out_filename\"\n",
    "        )\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"out_file_sz\",\n",
    "        file_size(\"out_file_path\")\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"is_out_file_valid\",\n",
    "        F\n",
    "          .when(F.isnull(\"out_file_sz\"), F.lit(False)) # <- null to False\n",
    "          .when(col(\"out_file_sz\") > F.lit(library.FILE_SIZE_THRESHOLD), F.lit(True))\n",
    "          .otherwise(F.lit(False))\n",
    "      )\n",
    "  )\n",
    "\n",
    "  # [4] merge changes back to original table\n",
    "  delta_tbl_eod = DeltaTable.forName(spark, band_tbl_name)\n",
    "\n",
    "  (\n",
    "    delta_tbl_eod.alias(\"eod\")\n",
    "      .merge(\n",
    "        df.alias(\"updates\"),\n",
    "        \"eod.item_id = updates.item_id\"\n",
    "      ) \n",
    "      .whenMatchedUpdate(\n",
    "        set = {\n",
    "          \"last_update\": \"updates.last_update\",\n",
    "          \"out_file_path\": \"updates.out_file_path\", \n",
    "          \"out_file_sz\": \"updates.out_file_sz\",\n",
    "          \"is_out_file_valid\": \"updates.is_out_file_valid\"\n",
    "        }\n",
    "      )\n",
    "    .execute()\n",
    "  ) \n",
    "  \n",
    "  # [5] return the changes\n",
    "  # - from the 'band_tbl_name'\n",
    "  return (\n",
    "    spark.table(band_tbl_name)\n",
    "      .filter(f\"last_update == '{last_updated}'\")\n",
    "  )\n",
    "\n",
    "def download_missing_assets(\n",
    "    band_tbl_name, where_clause=None, do_dry_run=False\n",
    "  ):\n",
    "  \"\"\"\n",
    "  Download missing assets for band (from pre-existing table) and update the table.\n",
    "  - Columns updated are 'out_file_sz', 'is_out_file_valid',\n",
    "      and 'last_update'\n",
    "  - Optional: 'where_clause' can be used to filter the table\n",
    "  - Missing is based on 'is_out_file_valid' being False or Null\n",
    "  - Assumes databricks catalog and schema already set\n",
    "  - default is to update vs dry-run\n",
    "  Returns the updated or dry run dataframe.\n",
    "  \"\"\"\n",
    "  # - df from table name\n",
    "  #   filter?\n",
    "  df = spark.table(band_tbl_name)\n",
    "  if where_clause is not None:\n",
    "    df = df.filter(where_clause)\n",
    "  \n",
    "  # - filter 'is_out_file_valid' is False or Null\n",
    "  df = df.filter(\n",
    "    (col(\"is_out_file_valid\") == False) |\n",
    "    (F.isnull(\"is_out_file_valid\"))\n",
    "  )\n",
    "  \n",
    "  if not do_dry_run:\n",
    "    # - handle missing\n",
    "    return update_assets(df, band_tbl_name)\n",
    "  else:\n",
    "    # - just df for dry-run\n",
    "    #   useful to test 'where_clase'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d9520e4-2736-4d4c-8b41-66db6fd42257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_A dry-run of a single item..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535305aa-d8c8-4539-a355-6b1371e37e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  download_missing_assets(\n",
    "    \"band_b03\", \n",
    "    where_clause=\"item_id = 'S2A_MSIL2A_20210628T221531_R115_T03VXC_20210630T063438'\", \n",
    "    do_dry_run=True\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c28f04c7-8933-4321-81bb-a70f3771a8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Actual run of a single item..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a209b38e-63dd-4386-90da-7f27ea37d7b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  download_missing_assets(\n",
    "    \"band_b03\", \n",
    "    where_clause=\"item_id = 'S2A_MSIL2A_20210628T221531_R115_T03VXC_20210630T063438'\", \n",
    "    do_dry_run=False \n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a605a70-e83e-4ff8-bf61-b0cee03cac45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_The `where_clause` param is optional in `download_missing_assets(...)`, when not specified, all data where 'is_out_file_valid' is not True will be tested and (re)downloaded as needed._ __Note: merges can be a more expensive, so do testing to see which option (including the one below), meets your needs.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2cdfa7-0e56-494b-8e22-6e7f9f148931",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download Any / All Other Bands\n",
    "\n",
    "> This call will get all the bands; it can be rerun to download any missing files also. __Note: `append_mode` is `False`, meaning it will overwrite the current band table, but we set `do_clean_files=False` so it will just download new / missing files.__ \n",
    "\n",
    "_This downloads ~10K GeoTIFFs per band for the state of Alaska. The operation can take some time, especially depending on (1) the size of your cluster, (2) whether starting fresh, and (3) how impacted you are by the free tier limits / throttling._ Also, sometimes an executor on the cluster might hang due to the nature of these longer running jobs (and the phased up delays from throttling). In the even that a task is hung up for say 15+ minutes, e.g. 511/512 tasks having completed, it is ok to interrupt and restart execution as files are first checked to see if they have already been downloaded, so the operation can recover without to much duplicated time / processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066b6f2b-9f2d-4560-9657-c7ce7b6f99d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# - uncomment when ready\n",
    "for band in ['B02', 'B03', 'B04', 'B08']:\n",
    "    print(f\"working on band '{band}'...\")\n",
    "    download_band(eod_item_df, band, False, do_clean_files=False, do_download=True, do_table_write=True)\n",
    "\n",
    "display(dbutils.fs.ls(f\"{EO_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d4a7985-cfb4-40a7-8d77-c7a511c33c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Verify we have all data for bands._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff00a71-25aa-430a-8a35-a71b3745653b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for band in ['B02', 'B03', 'B04', 'B08']:\n",
    "  print(f\"::: '{band}' :::\")\n",
    "  df_band = spark.table(f\"band_{band}\")\n",
    "  print(f\"\"\"\\ttotal?            {df_band.count():,}\"\"\")\n",
    "  print(f\"\"\"\\tis valid?         {df_band.filter(F.expr(\"is_out_file_valid = True\")).count():,}\"\"\")\n",
    "  print(f\"\"\"\\tnot valid?        {df_band.filter(F.expr(\"is_out_file_valid = False\")).count():,}\"\"\")\n",
    "  print(f\"\"\"\\tunknown validity? {df_band.filter(F.expr(\"is_out_file_valid is null\")).count():,}\"\"\")\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1655478-3994-4abd-9435-719fe10bb57e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_See all the tables generated._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740922a0-0a5e-4052-a6f9-154fffb8732d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a25b563-a260-48e9-bc3f-a2566dea73c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Look at the directories where the bands were downloaded._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85c1776-96cf-4189-bdf5-89c7af43a84b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(EO_DIR))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1974893261122030,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02. Download STACs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
