{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb894cb-bdef-4b7b-b91d-a92b0476e58b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb)\n",
    "\n",
    "\n",
    "# Document Normalizer annotator notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1198c557-3b2b-47b8-8951-1d96b1b05598",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Set up Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ecff456-e1f7-487a-8fad-fb55eb69e67a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b3533d0-56fd-4bfc-a931-126fd78dfc41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!wget http://ckl-it.de/wp-content/uploads/2022/12/docs.zip\n",
    "!unzip -f docs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b59d22c-5ecf-449b-9e62-9469d9a5a726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Start Spark NLP session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e36782-3bae-4c3b-b781-9130198eb737",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9146bf-c500-4e28-973f-c6bf711741e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Document Normalizer annotator overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb43822-5b67-415b-97a9-4c5431ae2083",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The DocumentNormalizer is an annotator that can be used after the DocumentAssembler to\n",
    "normalizes documents once that they have been processed and indexed.\n",
    "\n",
    "It takes in input annotated documents of type `Array[AnnotatorType]` (DOCUMENT) and gives\n",
    "as output annotated document of type AnnotatorType.DOCUMENT .\n",
    "\n",
    "Parameters are:\n",
    "- inputCol: input column name string which targets a column of type\n",
    "  Array(AnnotatorType.DOCUMENT).\n",
    "- outputCol: output column name string which targets a column of type\n",
    "  AnnotatorType.DOCUMENT.\n",
    "- action: action string to perform applying regex patterns, i.e. (clean | extract).\n",
    "  Default is \"clean\".\n",
    "- cleanupPatterns: normalization regex patterns which match will be removed from\n",
    "  document. Default is \"<[^>]*>\" (e.g., it removes all HTML tags).\n",
    "- replacement: replacement string to apply when regexes match. Default is \" \".\n",
    "- lowercase: whether to convert strings to lowercase. Default is False.\n",
    "- removalPolicy: removalPolicy to remove patterns from text with a given policy. Valid\n",
    "  policy values are: \"all\", \"pretty_all\", \"first\", \"pretty_first\". Defaults is\n",
    "  \"pretty_all\".\n",
    "- encoding: file encoding to apply on normalized documents. Supported encodings are:\n",
    "  UTF_8, UTF_16, US_ASCII, ISO-8859-1, UTF-16BE, UTF-16LE. Default is \"UTF-8\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50aa817c-0fd0-413f-9660-9dcc172dbaad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "inpuColName = \"document\"\n",
    "outputColName = \"normalizedDocument\"\n",
    "\n",
    "action = \"clean\"\n",
    "cleanUpPatterns = [\"<[^>]*>\"]\n",
    "replacement = \" \"\n",
    "removalPolicy = \"pretty_all\"\n",
    "encoding = \"UTF-8\"\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(inpuColName) \\\n",
    "    .setOutputCol(outputColName) \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(cleanUpPatterns) \\\n",
    "    .setReplacement(replacement) \\\n",
    "    .setPolicy(removalPolicy) \\\n",
    "    .setLowercase(True) \\\n",
    "    .setEncoding(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636d8711-af18-4de8-9bd8-83c83a431a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1697f280-bf97-40d6-8cf2-5aad700cca92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"html-docs\"\n",
    "\n",
    "data = spark.sparkContext.wholeTextFiles(\"html-docs\")\n",
    "df = data.toDF(schema=[\"filename\", \"text\"]).select(\"text\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c574ef1-4c8c-43ed-af97-b2f42d92cc8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example 1: remove all the tags from HTML text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab989e1b-0aa5-4170-b704-864752e0a7f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Once data is loaded we can process the textual document applying a pipeline that normalizes the document right after the DocumentAssembler.\n",
    "# For instance, let's imagine we are loading some HTML pages in our DataFrame and we want to remove all the tags in it:\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "cleanUpPatterns = [\"<[^>]*>\"]\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\") \\\n",
    "    .setAction(\"clean\") \\\n",
    "    .setPatterns(cleanUpPatterns) \\\n",
    "    .setReplacement(\" \") \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "      .setInputCols([\"normalizedDocument\"]) \\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "regexTokenizer = Tokenizer() \\\n",
    "      .setInputCols([\"sentence\"]) \\\n",
    "      .setOutputCol(\"token\") \\\n",
    "      .fit(df)\n",
    "\n",
    "docPatternRemoverPipeline = \\\n",
    "  Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        documentNormalizer,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer])\n",
    "\n",
    "ds = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "\n",
    "ds.select(\"normalizedDocument\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb36b28-10e7-463b-ae45-8f79afe1f78c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example 2: obfuscate PII such as emails in HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982e3223-160f-421b-ab96-b1aff3444a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "action = \"clean\"\n",
    "patterns = [\"([^.@\\\\s]+)(\\\\.[^.@\\\\s]+)*@([^.@\\\\s]+\\\\.)+([^.@\\\\s]+)\"]\n",
    "replacement = \"***OBFUSCATED PII***\"\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\") \\\n",
    "    .setAction(\"clean\") \\\n",
    "    .setPatterns(cleanUpPatterns) \\\n",
    "    .setReplacement(replacement) \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "      .setInputCols([\"normalizedDocument\"]) \\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "regexTokenizer = Tokenizer() \\\n",
    "      .setInputCols([\"sentence\"]) \\\n",
    "      .setOutputCol(\"token\") \\\n",
    "      .fit(df)\n",
    "\n",
    "docPatternRemoverPipeline = \\\n",
    "  Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        documentNormalizer,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer])\n",
    "\n",
    "ds = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "\n",
    "ds.select(\"normalizedDocument\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e3975a-8eb9-4fab-a996-c2d13b3bc1ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example 3: obfuscate PII such as ages in HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e84ba9e-1540-41ac-94d1-af25985aeaa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "action = \"clean\"\n",
    "patterns = [\"\\\\d+(?=[\\\\s]?year)\", \"(aged)[\\\\s]?\\\\d+\"]\n",
    "replacement = \"***OBFUSCATED PII***\"\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\") \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(patterns) \\\n",
    "    .setReplacement(replacement) \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "      .setInputCols([\"normalizedDocument\"]) \\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "regexTokenizer = Tokenizer() \\\n",
    "      .setInputCols([\"sentence\"]) \\\n",
    "      .setOutputCol(\"token\") \\\n",
    "      .fit(df)\n",
    "\n",
    "docPatternRemoverPipeline = \\\n",
    "  Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        documentNormalizer,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer])\n",
    "\n",
    "ds = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "\n",
    "ds.select(\"normalizedDocument\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca7c770-7ee3-491f-8dfc-c5051f97b3a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example 4: extract XML name tag contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbfab504-8553-4299-a737-699563dc7320",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "data = spark.sparkContext.wholeTextFiles(\"xml-docs\")\n",
    "df = data.toDF(schema=[\"filename\", \"text\"]).select(\"text\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b49447c2-7799-4e93-9adb-44126f03e4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "action = \"extract\"\n",
    "\n",
    "tag = \"name\"\n",
    "patterns = [tag]\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\") \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(patterns) \\\n",
    "    .setReplacement(\"\") \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "      .setInputCols([\"normalizedDocument\"]) \\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "regexTokenizer = Tokenizer() \\\n",
    "      .setInputCols([\"sentence\"]) \\\n",
    "      .setOutputCol(\"token\") \\\n",
    "      .fit(df)\n",
    "\n",
    "docPatternRemoverPipeline = \\\n",
    "  Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        documentNormalizer,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer])\n",
    "\n",
    "ds = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "\n",
    "ds.select(\"normalizedDocument\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3a63d7-cc11-4270-961a-1117e2ab375f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example 5 : apply lookaround patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5800d83f-8a8a-42da-a210-b1d52af227fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "articles = [\n",
    "            (1, \"10.2\",),\n",
    "            (2, \"9,53\",),\n",
    "            (3, \"11.01 mg\",),\n",
    "            (4, \"mg 11.01\",),\n",
    "            (5, \"14,220\",),\n",
    "            (6, \"Amoxiciline 4,5 mg for $10.35; Ibuprofen 5,5mg for $9.00.\",)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcb5b5ec-7e39-458a-8aec-6eca649c1eae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "articles_cols = [\"id\", \"text\"]\n",
    "df = spark.createDataFrame(data=articles, schema=articles_cols)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca0fb31-d850-4c84-86a9-f1a59f3c23c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Annotate replacing . to , using positive lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba14897-a046-4db9-a3c7-6b44462dd9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "# Targetting text 11.01 mg annotating to 11,01 mg\n",
    "\n",
    "action = \"lookaround\"\n",
    "patterns = [\".*\\d+(\\.)\\d+(?= mg).*\"]\n",
    "replacement = \",\"\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "doc_norm = DocumentNormalizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(patterns) \\\n",
    "    .setReplacement(replacement)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    doc_norm\n",
    "])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "model.transform(df).select(\"text\", \"normalized\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6175c6a-9f3b-4a5d-96ed-4f870ebb658c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Annotate replacing . to , using positive lookbehind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ae7c34-d637-4c23-8d87-287f1724a1eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Targetting text mg 11.01 annotating to mg 11,01\n",
    "\n",
    "action = \"lookaround\"\n",
    "patterns = [\".*(?<=mg )\\d+(\\.)\\d+.*\"]\n",
    "replacement = \",\"\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "doc_norm = DocumentNormalizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(patterns) \\\n",
    "    .setReplacement(replacement)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    doc_norm\n",
    "])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "model.transform(df).select(\"text\", \"normalized\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5feb77a8-9ec2-40e3-8df3-cd228b1d2022",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Annotate replacing , to . using iterative positive lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50f28b6-77f2-4386-b051-5c9e19c2d875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Targetting text Amoxiciline 4,5 mg for $10.35; Ibuprofen 5,5mg for $9.00.\n",
    "# annotating to \n",
    "# Amoxiciline 4.5 mg for $10.35; Ibuprofen 5.5mg for $9.00\n",
    "\n",
    "action = \"lookaround\"\n",
    "patterns = [\".*\\d+(\\,)\\d+(?=\\s?mg).*\"]\n",
    "replacement = \".\"\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "doc_norm = DocumentNormalizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setAction(action) \\\n",
    "    .setPatterns(patterns) \\\n",
    "    .setReplacement(replacement)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    doc_norm\n",
    "])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "model.transform(df).select(\"text\", \"normalized\").show(20, False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "document_normalizer_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
